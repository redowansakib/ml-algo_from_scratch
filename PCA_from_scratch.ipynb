{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d121d5-1154-4e9b-b95e-6bcd39f530fc",
   "metadata": {},
   "source": [
    "# PCA from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499fd34-98d4-4021-99b4-faab1582d5a7",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fe1f5-0042-4a8c-bb63-f2d2744565dc",
   "metadata": {},
   "source": [
    "PCA  i.e. Principal Component Analysis is a dimension reduction method commonly used in machine learning. It is commonly used for reducing the number of columns when there is thousands of columns whcih becomes exponentially diificult to manage with their increasing number. The naive idea to reduce column numbers is to cut off the columns but then we will have massive loss of information. Bur here PCA ensuers compact featurs(columns) with minimum loss of information.\n",
    "\n",
    "The main idea of PCA is to find a component onto which projection of data have maximum variance. For example if we want to reduce two columns $\\,x_1 \\,, x_2\\,$ in one then we need to plot each entry(blue dots) in a cartesian system with $x_1$ as abscissa and $x_2$ as ordiante where we try to find the best possible line(component) where the values have maximum variance while projected.<br>\n",
    "<div>\n",
    "    <center>\n",
    "        <img src=\"pca-example-1D-of-2D.png\" width=300>\n",
    "    </center>\n",
    "</div>\n",
    "<br>\n",
    "<center> Apparantly the line $u_1$ seems to quite fit for our purpose. Where each value $x^i$ is projected as $\\tilde{x}^i$ on $u_1$ with maximum variance. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc910f3a-a405-4da2-aa7c-2aa2b92a4d8a",
   "metadata": {},
   "source": [
    "## Perfroming PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce42f8-2fe2-4bd5-9c8a-6317c6a4a910",
   "metadata": {},
   "source": [
    "### Step-1: Get The Covariance matirx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d245956-cedb-4b79-864e-88dc5e2bd45b",
   "metadata": {},
   "source": [
    "\n",
    "First we get the covariance matrix for all the features in question.\n",
    "For any two variabel $x$ and $y$ we use this formula for covariance\n",
    "### $$cov_{x,y}=\\frac{\\sum(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{N-1}$$\n",
    "<center>\n",
    "    $cov_{x,y}$ = covariance between variable x and y <br>\n",
    "    $x_{i}$ = data value of x <br>\n",
    "    $y_{i}$ = data value of y <br>\n",
    "    $\\bar{x}$ = mean of x <br>\n",
    "    $\\bar{y}$ = mean of y <br>\n",
    "    $N$ = number of data values\n",
    "</center>\n",
    "<br>\n",
    "And a covarinace matrix is a $m \\times m\\,$ square matrix where any element $x_{ij}\\,$ is an element at row $i$ and column $j$ is covariance of features $\\,X_i\\,$ and $\\,X_j$\n",
    "\n",
    "#### $$ \\Sigma = \\begin{bmatrix}Var(X_1) & Cov(X_1, X_2) & \\ldots & Cov(X_1, X_{m}) \\\\\n",
    "                          Cov(X_1, X_2) & Var(X_2) & \\ldots & Cov(X_2, X_{m})\\\\\n",
    "                          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                          Cov(X_1,X_{m}) & Cov(X_2, X_{m}) &\\ldots & Var(X_{m})\\end{bmatrix}$$\n",
    "\n",
    "It turns out if we have a matrix $X$ of the data with the column of tha mtrix being each featue we can perform multiplication of $X^T$ and $X$ and divide every element with $N-1$ where $N$ is the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01a13c-0997-4d03-96b2-d90b971982d1",
   "metadata": {},
   "source": [
    "### Step-2: Get Eigenvectors of Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb4045-66b4-40f4-9b99-d5be7df3ca1c",
   "metadata": {},
   "source": [
    "Next we get the eigenvectors of the covariance matrix. For this we are going to use the Singular Value Decompositio or SVD. SVD of a matrix $X$ looks like this\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "Where $U$ and $V$ are two orthonormal matrices and $\\Sigma$ is a diagonal matrix of singular values of $X$.<br>\n",
    "<br>\n",
    "SVD of a square matrix acts as eigen decomposition which yields two matrices $P$ and $D$,\n",
    "$$ X = PDP^{-1} $$\n",
    "Where $ U = V = P$ and $ \\Sigma = D$. Here orthonormal matrices $P$ is the collection of unit eigenvectors of matrices $X$ and $D$ is a diagonal matrices with eigen values at its diagonal. The eigenvectors serve as the principal components where we can project our values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12044be6-6258-480d-8bd3-3bd93731c3d2",
   "metadata": {},
   "source": [
    "### Step-3: Project the Values onto the Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb68ee-d623-416d-a307-67fbca9e6473",
   "metadata": {},
   "source": [
    "Projection of any vector $x$ onto vector $y$ is given by,\n",
    "#### $$ proj_y(x) = \\frac{\\langle x,y \\rangle}{||y||^2} $$\n",
    "As we are going to project onto unit eigenvectors the norm of $y$ is $1$ or $||y||^2 = 1$. Which eases our calculations of $proj_y(x)$ to,\n",
    "#### $$ proj_y(x) = \\langle x,y \\rangle \\,  $$\n",
    "Inner product of $x$ and $y$ can be written as matrix multiplication of $x^T$ and $y$,\n",
    "#### $$ proj_y(x) = \\langle x,y \\rangle = x^T y = \\begin{bmatrix}\\ldots &x& \\ldots \\end{bmatrix} \\, \\begin{bmatrix}\\vdots \\\\ y \\\\ \\vdots \\end{bmatrix} $$\n",
    "We need to project all the entries of a data stored in matrix $X$ on the principal component.\n",
    "#### $$ proj_y(X) = \\begin{bmatrix}\\ldots &x_1& \\ldots \\\\ \\ldots &x_2& \\ldots \\\\ \\vdots&\\vdots&\\vdots \\\\ \\ldots &x_n& \\ldots \\end{bmatrix} \\, \\begin{bmatrix}\\vdots \\\\ y \\\\ \\vdots \\end{bmatrix} = X^T\\begin{bmatrix}\\vdots \\\\ y \\\\ \\vdots \\end{bmatrix}$$\n",
    "If there is several components then we project in all components which are contained in matrices $Y$\n",
    "#### $$ proj_y(X) = X^T \\begin{bmatrix} \\vdots&\\vdots&\\vdots&\\vdots \\\\ y_1&y_2&\\ldots&y_n \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\end{bmatrix} = X^TY $$\n",
    "\n",
    "And in the end the $\\,proj_y(X)\\,$ is our reduced dimension or columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd51dcb-8dba-4dad-b12a-9ecec2c10a66",
   "metadata": {},
   "source": [
    "### Let's Code PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f57def-2490-473f-8778-cbde7f81bc88",
   "metadata": {},
   "source": [
    "It's time we code up all the theories we have learned so far. <br>\n",
    "In order to implement Our PCA we will,<br>\n",
    "1. Calculate covariance matrix with the function `cov()` from `mystats`\n",
    "2. Calculate eigenvectors of covariance matrix by `svd()` from `mylinalg`\n",
    "3. Project each entry(row) onto the defined number of eigenvectros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535c9c0-8091-4945-91a4-68debde8c90e",
   "metadata": {},
   "source": [
    "N.B. `cov()` from `mystats` is coded from scratch depending on the idea that $cov(X) = \\frac{X^TX}{N-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363320f0-5ae6-4f18-bec9-5726816e874d",
   "metadata": {},
   "source": [
    "N.B. `matmul` and `svd()` from `mylinalg` is coded from scratch without help of any advance library. In order to undersatnd the implemntation of `svd()` please visit my linkedin post <a href=\"http://linekedin.com\" >Things I learnded while implementing PCA from scratch</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "151d837e-ad97-45df-b8fc-cb168b59d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mystats import cov\n",
    "from mylinalg import svd, matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd8fb25-21b7-428a-9890-319101b73d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPCA(data, n_components=1):\n",
    "    cov_mat = cov(data)\n",
    "    U, S, Vh = svd(cov_mat)\n",
    "    components = [U[i][:n_components] for i in range(len(U))]\n",
    "    reduced = matmul(data, components)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f14ef0-6b6d-41f7-a8af-81754ef445b6",
   "metadata": {},
   "source": [
    "Let's compare our `myPCA` with `PCA` from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a793b87-6601-4333-84b1-e35e5c5d70ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aeaddeb-32e3-4626-a472-e1da71e2d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer[\"data\"], columns=cancer[\"feature_names\"])\n",
    "scaled_data = StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0a995dc-4890-48e8-b421-8fae1c5f4567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components we want to reduce to\n",
    "n_components = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38a3c88-852a-4e55-b978-4116e1e45be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(scaled_data)\n",
    "x_pca = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f4a4aed-c6f2-46c8-b7d9-494983801ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca = myPCA(scaled_data.tolist(), n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5721d54-d97f-4f17-81a9-67ad22dd7376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction performed by 'sklearn.decomposition.PCA'\n",
      "[[ 9.19283683  1.94858308 -1.12316635]\n",
      " [ 2.3878018  -3.76817174 -0.52929295]\n",
      " [ 5.73389628 -1.0751738  -0.55174759]\n",
      " ...\n",
      " [ 1.25617928 -1.90229672  0.56273061]\n",
      " [10.37479406  1.6720101  -1.87702921]\n",
      " [-5.4752433  -0.67063678  1.49044265]]\n",
      "\n",
      "Reduction performed by our from scratch 'myPca'\n",
      "[-9.192837666695365, 1.948559386649288, -1.123162809958965]\n",
      "[-2.387800170472775, -3.768178337938722, -0.5292583364121393]\n",
      "[-5.7338958159750355, -1.0751882951008802, -0.5517348127949726]\n",
      "...\n",
      "[-1.2561784556313793, -1.90229861743443, 0.5627345368981741]\n",
      "[-10.374794780956703, 1.6719822147568666, -1.877053893158748]\n",
      "[5.475243591808819, -0.6706211475603386, 1.4904366899479098]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reduction performed by 'sklearn.decomposition.PCA'\")\n",
    "print(x_pca)\n",
    "print('')\n",
    "print(\"Reduction performed by our from scratch 'myPca'\")\n",
    "for r in my_pca[:3]:\n",
    "    print(r)\n",
    "print(\"...\")\n",
    "for r in my_pca[-3:]:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01add7fd-17fc-4fa0-b595-0ccba9c533d5",
   "metadata": {},
   "source": [
    "#### signs may be opposite for corresponding column due the reversal of eigenvector. Nonetheless the values work as expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
