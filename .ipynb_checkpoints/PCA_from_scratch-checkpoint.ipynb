{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d121d5-1154-4e9b-b95e-6bcd39f530fc",
   "metadata": {},
   "source": [
    "# PCA from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499fd34-98d4-4021-99b4-faab1582d5a7",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99fe1f5-0042-4a8c-bb63-f2d2744565dc",
   "metadata": {},
   "source": [
    "PCA  i.e. Principal Component Analysis is a dimension reduction method commonly used in machine learning. It is commonly used for reducing the number of columns when there is thousands of columns whcih becomes exponentially diificult to manage with their increasing number. The naive idea to reduce column numbers is to cut off the columns but then we will have massive loss of information. Bur here PCA ensuers compact featurs(columns) with minimum loss of information.\n",
    "\n",
    "The main idea of PCA is to find a component onto which projection of data have maximum variance. For example if we want to reduce two columns $\\,x_1 \\,, x_2\\,$ in one then we need to plot each entry(blue dots) in a cartesian system with $x_1$ as abscissa and $x_2$ as ordiante where we try to find the best possible line(component) where the values have maximum variance while projected.<br>\n",
    "<div>\n",
    "    <center>\n",
    "        <img src=\"pca-example-1D-of-2D.png\" width=300>\n",
    "    </center>\n",
    "</div>\n",
    "<br>\n",
    "<center> Apparantly the line $u_1$ seems to quite fit for our purpose. Where each value $x^i$ is projected as $\\tilde{x}^i$ on $u_1$ with maximum variance. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc910f3a-a405-4da2-aa7c-2aa2b92a4d8a",
   "metadata": {},
   "source": [
    "## Perfroming PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ce42f8-2fe2-4bd5-9c8a-6317c6a4a910",
   "metadata": {},
   "source": [
    "### Step-1: Get The Covariance matirx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d245956-cedb-4b79-864e-88dc5e2bd45b",
   "metadata": {},
   "source": [
    "\n",
    "First we get the covariance matrix for all the features in question.\n",
    "For any two variabel $x$ and $y$ we use this formula for covariance\n",
    "### $$cov_{x,y}=\\frac{\\sum(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{N-1}$$\n",
    "<center>\n",
    "    $cov_{x,y}$ = covariance between variable x and y <br>\n",
    "    $x_{i}$ = data value of x <br>\n",
    "    $y_{i}$ = data value of y <br>\n",
    "    $\\bar{x}$ = mean of x <br>\n",
    "    $\\bar{y}$ = mean of y <br>\n",
    "    $N$ = number of data values\n",
    "</center>\n",
    "<br>\n",
    "And a covarinace matrix is a $m \\times m\\,$ square matrix where any element $x_{ij}\\,$ is an element at row $i$ and column $j$ is covariance of features $\\,X_i\\,$ and $\\,X_j$\n",
    "\n",
    " $$ \\Sigma = \\begin{bmatrix}Var(X_1) & Cov(X_1, X_2) & \\ldots & Cov(X_1, X_{m}) \\\\\n",
    "                          Cov(X_1, X_2) & Var(X_2) & \\ldots & Cov(X_2, X_{m})\\\\\n",
    "                          \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                          Cov(X_1,X_{m}) & Cov(X_2, X_{m}) &\\ldots & Var(X_{m})\\end{bmatrix}$$\n",
    "\n",
    "It turns out if we have a matrix $X$ of the data with the column of tha matrices being each feature we can perform multiplication of $X^T$ and $X$ and divide every element with $N-1$ where $N$ is the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db01a13c-0997-4d03-96b2-d90b971982d1",
   "metadata": {},
   "source": [
    "### Step-2: Get Eigenvectors of Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb4045-66b4-40f4-9b99-d5be7df3ca1c",
   "metadata": {},
   "source": [
    "Next we get the eigenvectors of the covariance matrix. For this we are going to use the Singular Value Decompositio or SVD.\n",
    "#### What is SVD\n",
    "SVD is matices decomposition technique where we get left matrices $U$, diagonal matrix $\\Sigma$ and right matrices $V$. SVD of a $m \\times n$ matrix $X$ looks like this\n",
    "\n",
    "$$ X = U \\Sigma V^T $$\n",
    "If we expand the SVD decomposition we get,\n",
    "$$ X = \\begin{bmatrix} \n",
    "    \\vdots&\\vdots&\\vdots&\\vdots \\\\ \n",
    "    u_1&u_2&\\ldots&u_m \\\\\n",
    "    \\vdots&\\vdots&\\vdots&\\vdots \\end{bmatrix}\n",
    "    \\begin{bmatrix} \n",
    "    \\sigma_1&0&0&0&\\ldots&0\\\\ \n",
    "    0&\\ddots&0&\\vdots&\\quad&\\vdots \\\\\n",
    "    0&0&\\sigma_m&0&\\ldots&0 \\end{bmatrix}\n",
    "    \\begin{bmatrix} \n",
    "    \\ldots&v_1&\\ldots \\\\\n",
    "    \\ldots&v_2&\\ldots \\\\\n",
    "    \\ldots&\\vdots&\\ldots \\\\\n",
    "    \\ldots&v_n&\\ldots\n",
    "    \\end{bmatrix}               \n",
    "    $$\n",
    "$$ X = u_1\\sigma_1 v_1 + u_2\\sigma_2 v_2 + \\ldots + u_m\\sigma_m v_m $$\n",
    "$$ X = \\sum_{i=1}^{m} u_i\\sigma_i v_i$$\n",
    "<br>\n",
    "\n",
    "Where $U$ and $V$ are two orthonormal matrices and $\\Sigma$ is a diagonal matrix. $U$ is a collection of eigen vectors obtained from $XX^T$ and $V$ is a collection of eigen vectors obtained from $X^TX$ and $\\Sigma$ contains singular values of $X$. \n",
    "\n",
    "<br>\n",
    "As $V^T$ is a orthonormal matrices we can say $V^T = V^{-1}$ and write the SVD equation as $X = U \\Sigma V^T = U \\Sigma V^{-1}$ multiplying both sides with $V$ we get $XV = U \\Sigma$. As $\\Sigma$ is a diagonal matrix for every vector $u_i$ in matrices $U$ and every vector $v_i$ in matrices $V$ we can write that\n",
    "$$ Xv_i = \\sigma_i u_i$$\n",
    "<br>\n",
    "\n",
    "So, if we can determine every eigenvector $v_i$ of $X^TX$  we can get every $\\sigma_i \\,$ and $ u_i$ vector.\n",
    "<br>\n",
    "#### Power Iteration\n",
    "In order to get eigenvector of any matrices we can power iterarate any random vector until it ocnverges to any eigenvector. For any matrices $A$ we can pick any random vector $k$ and multipy the vector with increasing power $A$ until we find that $kA^n = kA^{n+1}$ where $kA^n$ is eigen vector of $A$.<br>\n",
    "\n",
    "How we write the code for it? For mtrices $A$ we choose any random vector $k$, we multipy it with our matrices $A$ and get vector $k_1$ and again multipy $k_1$ with matrices $A$. We keep multiplying in this manner until the multiplication doesnt change the vector. Such that, \n",
    "$$ a_i = A\\,a_{i+1} $$\n",
    "The vector $a_i$ unaffected by the transformtion of matrices $A$ is by defintition is an eigen vector of $A$. \n",
    "<br>\n",
    "#### Establishing SVD\n",
    "We power iterate through  $X^TX$ and obtain $v_i$ a vector of $V$. After obtaining a vector $v_i$ we use $ Xv_i = \\sigma_i u_i$ to get $\\sigma \\,$ and $ u_i$. We can exploit that $ X = \\sum_{i=1}^{m} u_i\\sigma_i v_i$ and strip the X of one $u_i\\sigma_i v_i$ and then power iterate again until we extract all the vectors $v_i$ and corresponding $\\sigma_i$ and $u_i$.\n",
    "<br>\n",
    "#### Eigenvector of a Square Matrices\n",
    "SVD of a square matrix acts as eigen decomposition which yields two matrices $P$ and $D$,\n",
    "$$ X = PDP^{-1} $$\n",
    "Where $ U = V = P$ and $ \\Sigma = D$. Here orthonormal matrices $P$ is the collection of unit eigenvectors of matrices $X$ and $D$ is a diagonal matrices with eigen values at its diagonal. The eigenvectors serve as the principal components where we can project our values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12044be6-6258-480d-8bd3-3bd93731c3d2",
   "metadata": {},
   "source": [
    "### Step-3: Project the Values onto the Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb68ee-d623-416d-a307-67fbca9e6473",
   "metadata": {},
   "source": [
    "Projection of any vector $x$ onto vector $y$ is given by,\n",
    "#### $$ proj_y(x) = \\frac{\\langle x,y \\rangle}{||y||^2} $$\n",
    "As we are going to project onto unit eigenvectors the norm of $y$ is $1$ or $||y||^2 = 1$. Which eases our calculations of $proj_y(x)$ to,\n",
    "#### $$ proj_y(x) = \\langle x,y \\rangle \\,  $$\n",
    "Inner product of $x$ and $y$ can be written as matrix multiplication of $x^T$ and $y$,\n",
    "#### $$ proj_y(x) = \\langle x,y \\rangle = x^T y = \\begin{bmatrix}\\ldots &x& \\ldots \\end{bmatrix} \\, \\begin{bmatrix}\\vdots \\\\ y \\\\ \\vdots \\end{bmatrix} $$\n",
    "We need to project all the entries of a data stored in matrix $X$ on the principal component.\n",
    "#### $$ proj_y(X) = \\begin{bmatrix}\\ldots &x_1& \\ldots \\\\ \\ldots &x_2& \\ldots \\\\ \\vdots&\\vdots&\\vdots \\\\ \\ldots &x_n& \\ldots \\end{bmatrix} \\, \\begin{bmatrix}\\vdots \\\\ y \\\\ \\vdots \\end{bmatrix} = X^T\\begin{bmatrix}\\vdots \\\\ y \\\\ \\vdots \\end{bmatrix}$$\n",
    "If there is several components then we project in all components which are contained in matrices $Y$\n",
    "#### $$ proj_y(X) = X^T \\begin{bmatrix} \\vdots&\\vdots&\\vdots&\\vdots \\\\ y_1&y_2&\\ldots&y_n \\\\ \\vdots&\\vdots&\\vdots&\\vdots \\end{bmatrix} = X^TY $$\n",
    "\n",
    "And in the end the $\\,proj_y(X)\\,$ is our reduced dimension or columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd51dcb-8dba-4dad-b12a-9ecec2c10a66",
   "metadata": {},
   "source": [
    "### Let's Code PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f57def-2490-473f-8778-cbde7f81bc88",
   "metadata": {},
   "source": [
    "It's time we code up all the theories we have learned so far. <br>\n",
    "In order to implement Our PCA we will,<br>\n",
    "1. Calculate covariance matrix with the function `cov()` from `mystats`\n",
    "2. Calculate eigenvectors of covariance matrix by `svd()` from `mylinalg`\n",
    "3. Project each entry(row) onto the defined number of eigenvectros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7535c9c0-8091-4945-91a4-68debde8c90e",
   "metadata": {},
   "source": [
    "N.B. <a href=https://github.com/redowansakib/ml-algo_from_scratch/blob/cda1e1eda4a023865bef289d553a65aafcf737a4/mystats.py#L109-L120>`cov()`</a> from <a href=https://github.com/redowansakib/ml-algo_from_scratch/blob/cda1e1eda4a023865bef289d553a65aafcf737a4/mystats.py>`mystats`</a> is coded from scratch adapting the idea that $cov(X) = \\frac{X^TX}{N-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363320f0-5ae6-4f18-bec9-5726816e874d",
   "metadata": {},
   "source": [
    "N.B. <a href=https://github.com/redowansakib/ml-algo_from_scratch/blob/cda1e1eda4a023865bef289d553a65aafcf737a4/mylinalg.py#L259-L290>`matmul()`</a> and <a href=https://github.com/redowansakib/ml-algo_from_scratch/blob/cda1e1eda4a023865bef289d553a65aafcf737a4/mylinalg.py#L362-L378>`svd()`</a> are from <a href=https://github.com/redowansakib/ml-algo_from_scratch/blob/cda1e1eda4a023865bef289d553a65aafcf737a4/mylinalg.py>`mylinalg`</a> which is coded from scratch without help of any advance library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "151d837e-ad97-45df-b8fc-cb168b59d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mystats import cov\n",
    "from mylinalg import svd, matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bd8fb25-21b7-428a-9890-319101b73d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myPCA(data, n_components=1):\n",
    "    cov_mat = cov(data)\n",
    "    U, S, Vh = svd(cov_mat)\n",
    "    components = [U[i][:n_components] for i in range(len(U))]\n",
    "    reduced = matmul(data, components)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f14ef0-6b6d-41f7-a8af-81754ef445b6",
   "metadata": {},
   "source": [
    "Let's compare our `myPCA` with `PCA` from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a793b87-6601-4333-84b1-e35e5c5d70ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4aeaddeb-32e3-4626-a472-e1da71e2d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(cancer[\"data\"], columns=cancer[\"feature_names\"])\n",
    "scaled_data = StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0a995dc-4890-48e8-b421-8fae1c5f4567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of components we want to reduce to\n",
    "n_components = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b38a3c88-852a-4e55-b978-4116e1e45be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(scaled_data)\n",
    "x_pca = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f4a4aed-c6f2-46c8-b7d9-494983801ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca = myPCA(scaled_data.tolist(), n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5721d54-d97f-4f17-81a9-67ad22dd7376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction performed by 'sklearn.decomposition.PCA'\n",
      "[[ 9.19283683  1.94858306 -1.12316656]\n",
      " [ 2.3878018  -3.76817175 -0.52929324]\n",
      " [ 5.73389628 -1.0751738  -0.55174758]\n",
      " ...\n",
      " [ 1.25617928 -1.90229671  0.56273069]\n",
      " [10.37479406  1.67201011 -1.87702908]\n",
      " [-5.4752433  -0.6706368   1.49044235]]\n",
      "\n",
      "Reduction performed by our from scratch 'myPca'\n",
      "[9.192839156136902, 1.948521477076008, -1.1231983692049408]\n",
      "[2.38779729014697, -3.7681881071846837, -0.5292751139732903]\n",
      "[5.733894994123107, -1.0752119852580972, -0.5517458345277453]\n",
      "...\n",
      "[1.2561770015575673, -1.9023040250680678, 0.5627539600875417]\n",
      "[10.37479605898259, 1.6719396519313308, -1.877026923506224]\n",
      "[-5.475244104411029, -0.6705988383792829, 1.4904599194067307]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reduction performed by 'sklearn.decomposition.PCA'\")\n",
    "print(x_pca)\n",
    "print('')\n",
    "print(\"Reduction performed by our from scratch 'myPca'\")\n",
    "for r in my_pca[:3]:\n",
    "    print(r)\n",
    "print(\"...\")\n",
    "for r in my_pca[-3:]:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
